{"learning_rate": 0.006781858052406466, "batch_size": 595, "dropout": 0.37492320152163416, "l2_reg": 0.0011306201367083015, "anneal_cap": 0.23854976231287403, "encoding_size": 960, "next_layer_size_multiplier": 3, "total_anneal_steps": 21971}